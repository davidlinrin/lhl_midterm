{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### #Data management and manipulation ####\n",
    "import pandas as pd # import pandas\n",
    "import numpy as np # import pandas\n",
    "import random\n",
    "\n",
    "\n",
    "#### Machine Learning model building/evaluation ###\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/Users/carlyfennell/DataScience/data_bootcamp/data/'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "column_types={'mkt_carrier_fl_num': 'uint16', 'op_unique_carrier': 'category', 'tail_num': 'category', 'op_carrier_fl_num': 'uint16', 'origin_airport_id': 'uint16', 'origin': 'category', 'dest_airport_id': 'uint16', 'dest': 'category', 'crs_dep_time': 'uint16', 'dep_time': 'float32'}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_flights=pd.read_csv(path+'flights.csv',dtype=column_types,parse_dates=['fl_date'],infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_flights['fl_date']=df_flights['fl_date'].astype('datetime64[ns]')\n",
    "df_flights['month']=df_flights['fl_date'].dt.month\n",
    "df_flights['day']=df_flights['fl_date'].dt.day"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_flights.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_flights=df_flights.drop(['op_unique_carrier','mkt_carrier','dep_time','taxi_out', 'wheels_off', 'wheels_on', 'taxi_in','arr_time','no_name','first_dep_time', 'total_add_gtime',\n",
    "       'longest_add_gtime','diverted', 'dup', 'crs_elapsed_time',\n",
    "       'actual_elapsed_time', 'air_time', 'flights'],axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#read weather_key into dataframe\n",
    "weather_key=pd.read_csv(path+'wkey.csv')\n",
    "weather_key=weather_key.drop(['Unnamed: 0','period'],axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "weather_key.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge weather to flights"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_flights.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "new_df=pd.merge(df_flights,weather_key[['origin','month', 'wdir', 'temp',\n",
    "       'maxt', 'visibility', 'wspd', 'heatindex', 'cloudcover', 'mint',\n",
    "       'precip', 'snowdepth', 'sealevelpressure', 'dew', 'humidity', 'wgust',\n",
    "       'precipcover', 'windchill', 'closest_airport']],how='outer',on=['origin','month'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge aggregate traffic to new df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "busy_feat=pd.read_csv(path+'busy_feature.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "busy_feat=busy_feat.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "busy_feat.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "busy_feat.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "new_df_two=pd.merge(new_df,busy_feat,how='outer',on=['origin'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#save to csv\n",
    "new_df_two.to_csv(path+'flight_N_weath.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "new_df_two.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge in David's features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "flights_final is just any flights table with the dep_delay, arr_delay, and tail_num"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import copy\n",
    "flights_FE = copy.deepcopy(new_df_two)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "flights_FE = flights_FE[(flights_FE['dep_delay']>0) | (flights_FE['arr_delay']>0)]\n",
    "delay_df = flights_FE.groupby('tail_num').mean()[['arr_delay','dep_delay']]\n",
    "delay_df.reset_index(inplace=True)\n",
    "delay_df.columns = ['tail_num','mean_tail_num_arr_delay','mean_tail_num_dep_delay']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "flights_final = new_df_two.merge(delay_df, on=['tail_num'], how='left')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "carrier_delay = flights_FE.groupby('mkt_unique_carrier').mean()[['arr_delay','dep_delay']]\n",
    "carrier_delay.reset_index(inplace=True)\n",
    "carrier_delay.columns = ['mkt_unique_carrier','mean_carrier_arr_delay','mean_carrier__dep_delay']\n",
    "flights_final = flights_final.merge(carrier_delay, on=['mkt_unique_carrier'], how='left')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#save to csv\n",
    "flights_final.to_csv(path+'flight_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types={'mkt_carrier_fl_num': 'float64', 'distance': 'float64', 'month': 'int64', 'day': 'float64', 'wdir': 'float64', 'temp': 'float64', 'visibility': 'float64', 'wspd': 'float64', 'heatindex': 'float64', 'cloudcover': 'float64'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bootcamp_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (18) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "flights_final=pd.read_csv(path+'flight_final.csv',dtype=column_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=flights_final[['cancelled','mkt_carrier_fl_num','distance', 'month', 'day','origin',\n",
    "                              'dest', 'wdir', 'temp','visibility', 'wspd', 'heatindex', 'cloudcover','precip',\n",
    "                              'sealevelpressure', 'humidity', 'wgust','precipcover', 'windchill','total_flights','snowdepth',\n",
    "                              'branded_code_share','mean_tail_num_arr_delay','mean_carrier_arr_delay','crs_dep_time','crs_arr_time']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['snowdepth']=X['snowdepth'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>windchill</th>\n",
       "      <td>5120122</td>\n",
       "      <td>0.321462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heatindex</th>\n",
       "      <td>4817126</td>\n",
       "      <td>0.302439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_tail_num_arr_delay</th>\n",
       "      <td>49448</td>\n",
       "      <td>0.003105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sealevelpressure</th>\n",
       "      <td>12139</td>\n",
       "      <td>0.000762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wgust</th>\n",
       "      <td>8126</td>\n",
       "      <td>0.000510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crs_arr_time</th>\n",
       "      <td>105</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distance</th>\n",
       "      <td>105</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>105</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dest</th>\n",
       "      <td>105</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crs_dep_time</th>\n",
       "      <td>105</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Total   Percent\n",
       "windchill                5120122  0.321462\n",
       "heatindex                4817126  0.302439\n",
       "mean_tail_num_arr_delay    49448  0.003105\n",
       "sealevelpressure           12139  0.000762\n",
       "wgust                       8126  0.000510\n",
       "crs_arr_time                 105  0.000007\n",
       "distance                     105  0.000007\n",
       "day                          105  0.000007\n",
       "dest                         105  0.000007\n",
       "crs_dep_time                 105  0.000007"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = X.isnull().sum().sort_values(ascending=False)\n",
    "percent = (X.isnull().sum()/X.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['windchill', 'heatindex', 'mean_tail_num_arr_delay', 'sealevelpressure',\n",
       "       'wgust', 'crs_arr_time', 'distance', 'day', 'dest', 'crs_dep_time',\n",
       "       'mkt_carrier_fl_num', 'branded_code_share', 'mean_carrier_arr_delay',\n",
       "       'humidity', 'precipcover', 'cloudcover', 'total_flights', 'wspd',\n",
       "       'visibility', 'temp', 'wdir', 'snowdepth', 'origin', 'month', 'precip'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "windchill    25.7\n",
       "heatindex    95.0\n",
       "wgust        44.7\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[['windchill', 'heatindex','wgust']].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['windchill']=X['windchill'].fillna(25.7)\n",
    "X['heatindex']=X['heatindex'].fillna(95)\n",
    "X['wgust']=X['wgust'].fillna(44.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.sample(n=8000000, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bin a few vars"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "flight_balanced['branded_code_share'] = pd.factorize(flight_balanced['branded_code_share'])[0]\n",
    "flight_balanced['mkt_carrier_fl_num'] = pd.factorize(flight_balanced['mkt_carrier_fl_num'])[0]\n",
    "flight_balanced['origin'] = pd.factorize(flight_balanced['origin'])[0]\n",
    "flight_balanced['dest'] = pd.factorize(flight_balanced['dest'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter and change values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_num['snowdepth'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_cat=X[list(X.dtypes[X.dtypes=='object'].index)]\n",
    "#X_num=X[list(X.dtypes[X.dtypes!='object'].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat=X[['origin', 'dest', 'branded_code_share','mkt_carrier_fl_num']]\n",
    "X_num=X.drop(['origin', 'dest', 'branded_code_share','mkt_carrier_fl_num'],axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "weather_key.loc[(weather_key['origin']==weather_key['origin'][i]) & (weather_key['month']==weather_key['month'][i]), aw_vars]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_num.loc[X_num['snowdepth']<1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bootcamp_env/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/anaconda3/envs/bootcamp_env/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/envs/bootcamp_env/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/anaconda3/envs/bootcamp_env/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "X_cat['branded_code_share'] = pd.factorize(X_cat['branded_code_share'])[0]\n",
    "X_cat['mkt_carrier_fl_num'] = pd.factorize(X_cat['mkt_carrier_fl_num'])[0]\n",
    "X_cat['origin'] = pd.factorize(X_cat['origin'])[0]\n",
    "X_cat['dest'] = pd.factorize(X_cat['dest'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bootcamp_env/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_num)\n",
    "X_std=scaler.transform(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mkt_carrier_fl_num', 'distance', 'month', 'day', 'origin', 'dest',\n",
       "       'wdir', 'temp', 'visibility', 'wspd', 'heatindex', 'cloudcover',\n",
       "       'precip', 'sealevelpressure', 'humidity', 'wgust', 'precipcover',\n",
       "       'windchill', 'total_flights', 'snowdepth', 'branded_code_share',\n",
       "       'mean_tail_num_arr_delay', 'mean_carrier_arr_delay', 'crs_dep_time',\n",
       "       'crs_arr_time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15927590, 49)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=flights_final['cancelled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2=pd.concat([X_num,X_cat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### SPLIT DATA #######\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_2, y, test_size=0.3,random_state=0) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Class']=y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype in ['float','int','object']:\n",
    "    selected_dtype = X_train.select_dtypes(include=[dtype])\n",
    "    mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n",
    "    mean_usage_mb = mean_usage_b / 1024 ** 2\n",
    "    print(\"Average memory usage for {} columns: {:03.2f} MB\".format(dtype,mean_usage_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to be calculating memory usage a lot,\n",
    "# so we'll create a function to save us some time!\n",
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### integers\n",
    "gl_int = X_train.select_dtypes(include=['int'])\n",
    "converted_int = gl_int.apply(pd.to_numeric,downcast='unsigned')\n",
    "print(mem_usage(gl_int))\n",
    "print(mem_usage(converted_int))\n",
    "compare_ints = pd.concat([gl_int.dtypes,converted_int.dtypes],axis=1)\n",
    "compare_ints.columns = ['before','after']\n",
    "compare_ints.apply(pd.Series.value_counts)\n",
    "\n",
    "### floats\n",
    "gl_float = X_train.select_dtypes(include=['float'])\n",
    "converted_float = gl_float.apply(pd.to_numeric,downcast='float')\n",
    "print(mem_usage(gl_float))\n",
    "print(mem_usage(converted_float))\n",
    "compare_floats = pd.concat([gl_float.dtypes,converted_float.dtypes],axis=1)\n",
    "compare_floats.columns = ['before','after']\n",
    "compare_floats.apply(pd.Series.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Optimize for numeric\n",
    "optimized_gl = X_train.copy()\n",
    "optimized_gl[converted_int.columns] = converted_int\n",
    "optimized_gl[converted_float.columns] = converted_float\n",
    "print(mem_usage(X_train))\n",
    "print(mem_usage(optimized_gl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_obj = X_train.select_dtypes(include=['object']).copy()\n",
    "gl_obj.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_obj = pd.DataFrame()\n",
    "for col in gl_obj.columns:\n",
    "    num_unique_values = len(gl_obj[col].unique())\n",
    "    num_total_values = len(gl_obj[col])\n",
    "    if num_unique_values / num_total_values < 0.5:\n",
    "        converted_obj.loc[:,col] = gl_obj[col].astype('category')\n",
    "    else:\n",
    "        converted_obj.loc[:,col] = gl_obj[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mem_usage(gl_obj))\n",
    "print(mem_usage(converted_obj))\n",
    "compare_obj = pd.concat([gl_obj.dtypes,converted_obj.dtypes],axis=1)\n",
    "compare_obj.columns = ['before','after']\n",
    "compare_obj.apply(pd.Series.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[converted_obj.columns] = converted_obj\n",
    "mem_usage(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = X_train.dtypes\n",
    "dtypes_col = dtypes.index\n",
    "dtypes_type = [i.name for i in dtypes.values]\n",
    "column_types = dict(zip(dtypes_col, dtypes_type))\n",
    "# rather than print all 161 items, we'll\n",
    "# sample 10 key/value pairs from the dict\n",
    "# and print it nicely using prettyprint\n",
    "preview = first2pairs = {key:value for key,value in list(column_types.items())[:10]}\n",
    "#import pprintpp\n",
    "#pp = pp = pprint.PrettyPrinter(indent=4)\n",
    "print(preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types={'mkt_carrier_fl_num': 'float64', 'distance': 'float64', 'month': 'int64', 'day': 'float64', 'wdir': 'float64', 'temp': 'float64', 'visibility': 'float64', 'wspd': 'float64', 'heatindex': 'float64', 'cloudcover': 'float64'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.countplot(df_flights['cancelled'])\n",
    "df_flights['cancelled'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try oversampling with resample from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import seaborn as sns\n",
    "sns.countplot(X_train['Class'])\n",
    "X_train['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancelled=X_train[X_train['Class']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a value for n_samples that will make class 1 even up\n",
    "# prepare bootstrap sample\n",
    "boot = resample(cancelled, replace=True, n_samples=4000000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "balanced_data=pd.concat([X_train,boot])\n",
    "y_bal=balanced_data['Class']\n",
    "X_bal=balanced_data.drop(['Class'],axis=1)\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf_bal = BaggingClassifier(n_estimators=10, random_state=0)\n",
    "clf_bal.fit(X_bal, y_bal)\n",
    "y_pred_bal_2=clf_bal.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred_bal_2)\n",
    "cnf_matrix\n",
    "\n",
    "print (\"accuracy: \", metrics.accuracy_score(y_test, y_pred_bal_2))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred_bal_2))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred_bal_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used this tutorial as a starting point\n",
    "\n",
    "https://machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter data frame where class==fradulent\n",
    "filter data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "492, 284315\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train['Class']=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(X_train['Class'])\n",
    "X_train['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.groupby('Class').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter based on label into just fraudulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancelled=X_train[X_train['Class']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run resampling on the fraudsters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare bootstrap sample\n",
    "boot = resample(cancelled, replace=True, n_samples=4000000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot.groupby('Class').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append Bootstrapped fraudulent cases to original X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data=pd.concat([X_train,boot])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "balanced_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data.groupby('Class').count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You can see that the bootstrapping worked and now we have a relatively even distribution of fraudulent vs unfraudulent cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-splice off the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bal=balanced_data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bal=balanced_data.drop(['Class'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#save to csv\n",
    "y_bal.to_csv(path+'y_bal.csv')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_bal.to_csv(path+'X_bal.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.read_csv(path+'y_bal.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.read_csv(path+'X_bal.csv',dtype=column_types,parse_dates=['fl_date'],infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with this data set"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bal_model = GaussianNB().fit(X_bal, y_bal) #fitting our model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_pred_bal=bal_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred_bal)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print (\"accuracy: \", metrics.accuracy_score(y_test, y_pred_bal))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred_bal))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred_bal))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We are getting terrible recall and precision because of imbalance in classes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "auc = metrics.roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "WORSE THAN UNBALANCED - try with a different algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf_bal = BaggingClassifier(n_estimators=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bal.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_conca=pd.concat([X_bal,y_bal],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_conca=X_conca.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_conca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_conca.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bal=X_conca['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bal=X_conca.drop('Class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(X_bal['mkt_carrier_fl_num'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bal=X_bal.drop(['origin','dest','branded_code_share'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_bal.fit(X_bal, y_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.drop(['origin','dest','branded_code_share'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf_bal.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"accuracy: \", metrics.accuracy_score(y_test, y_pred_bal_2))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred_bal_2))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred_bal_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_env",
   "language": "python",
   "name": "bootcamp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
